moisioa3@smith ~/snlp-project/word_language_model
 % python3 main.py --cuda --data yle --emmodel ../data/embeddings/Word2Vec_iltalehti-wikipedia_new_size=200,alpha=0.025,window=10,min_count=2,sg=1,negative=5,iter=10.bin --save w2v_wp-il_non_norm.pt
using pretrained word embeddings ../data/embeddings/Word2Vec_iltalehti-wikipedia_new_size=200,alpha=0.025,window=10,min_count=2,sg=1,negative=5,iter=10.bin
encoder layer shape torch.Size([379997, 200])
OOV count 337438
| epoch   1 |   200/ 2108 batches | lr 20.00 | ms/batch 166.52 | loss 11.23 | ppl 75645.54
| epoch   1 |   400/ 2108 batches | lr 20.00 | ms/batch 166.58 | loss 10.51 | ppl 36728.58
| epoch   1 |   600/ 2108 batches | lr 20.00 | ms/batch 166.88 | loss 10.17 | ppl 26006.25
| epoch   1 |   800/ 2108 batches | lr 20.00 | ms/batch 167.16 | loss 10.01 | ppl 22324.07
| epoch   1 |  1000/ 2108 batches | lr 20.00 | ms/batch 167.77 | loss  9.94 | ppl 20745.11
| epoch   1 |  1200/ 2108 batches | lr 20.00 | ms/batch 167.74 | loss  9.75 | ppl 17198.52
| epoch   1 |  1400/ 2108 batches | lr 20.00 | ms/batch 167.57 | loss  9.68 | ppl 15953.07
| epoch   1 |  1600/ 2108 batches | lr 20.00 | ms/batch 167.80 | loss  9.58 | ppl 14488.22
| epoch   1 |  1800/ 2108 batches | lr 20.00 | ms/batch 167.58 | loss  9.49 | ppl 13230.27
| epoch   1 |  2000/ 2108 batches | lr 20.00 | ms/batch 167.73 | loss  9.50 | ppl 13325.33
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 375.60s | valid loss  9.49 | valid ppl 13174.51
-----------------------------------------------------------------------------------------
| epoch   2 |   200/ 2108 batches | lr 20.00 | ms/batch 168.80 | loss  9.39 | ppl 11953.64
| epoch   2 |   400/ 2108 batches | lr 20.00 | ms/batch 167.70 | loss  9.29 | ppl 10798.25
| epoch   2 |   600/ 2108 batches | lr 20.00 | ms/batch 167.69 | loss  9.04 | ppl  8457.49
| epoch   2 |   800/ 2108 batches | lr 20.00 | ms/batch 167.79 | loss  9.16 | ppl  9531.57
| epoch   2 |  1000/ 2108 batches | lr 20.00 | ms/batch 167.58 | loss  9.18 | ppl  9744.95
| epoch   2 |  1200/ 2108 batches | lr 20.00 | ms/batch 167.76 | loss  9.08 | ppl  8756.28

| epoch   2 |  1400/ 2108 batches | lr 20.00 | ms/batch 167.72 | loss  9.01 | ppl  8185.00
| epoch   2 |  1600/ 2108 batches | lr 20.00 | ms/batch 167.63 | loss  8.95 | ppl  7683.95
| epoch   2 |  1800/ 2108 batches | lr 20.00 | ms/batch 167.69 | loss  8.91 | ppl  7389.30
| epoch   2 |  2000/ 2108 batches | lr 20.00 | ms/batch 167.95 | loss  8.94 | ppl  7660.82
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 376.57s | valid loss  9.31 | valid ppl 11044.94
-----------------------------------------------------------------------------------------
| epoch   3 |   200/ 2108 batches | lr 20.00 | ms/batch 168.64 | loss  8.89 | ppl  7248.30
| epoch   3 |   400/ 2108 batches | lr 20.00 | ms/batch 167.48 | loss  8.81 | ppl  6681.21
| epoch   3 |   600/ 2108 batches | lr 20.00 | ms/batch 168.55 | loss  8.55 | ppl  5172.33
| epoch   3 |   800/ 2108 batches | lr 20.00 | ms/batch 167.66 | loss  8.74 | ppl  6235.48
| epoch   3 |  1000/ 2108 batches | lr 20.00 | ms/batch 168.44 | loss  8.76 | ppl  6370.50
| epoch   3 |  1200/ 2108 batches | lr 20.00 | ms/batch 168.13 | loss  8.67 | ppl  5832.40
| epoch   3 |  1400/ 2108 batches | lr 20.00 | ms/batch 169.08 | loss  8.60 | ppl  5423.60
| epoch   3 |  1600/ 2108 batches | lr 20.00 | ms/batch 169.50 | loss  8.54 | ppl  5107.04
| epoch   3 |  1800/ 2108 batches | lr 20.00 | ms/batch 170.28 | loss  8.51 | ppl  4968.76
| epoch   3 |  2000/ 2108 batches | lr 20.00 | ms/batch 168.55 | loss  8.56 | ppl  5202.48
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 378.35s | valid loss  9.09 | valid ppl  8832.67
-----------------------------------------------------------------------------------------
| epoch   4 |   200/ 2108 batches | lr 20.00 | ms/batch 171.12 | loss  8.52 | ppl  5013.06
| epoch   4 |   400/ 2108 batches | lr 20.00 | ms/batch 170.11 | loss  8.45 | ppl  4667.62
| epoch   4 |   600/ 2108 batches | lr 20.00 | ms/batch 170.88 | loss  8.19 | ppl  3613.58
| epoch   4 |   800/ 2108 batches | lr 20.00 | ms/batch 169.97 | loss  8.40 | ppl  4456.55
| epoch   4 |  1000/ 2108 batches | lr 20.00 | ms/batch 170.12 | loss  8.43 | ppl  4569.97
| epoch   4 |  1200/ 2108 batches | lr 20.00 | ms/batch 170.25 | loss  8.34 | ppl  4206.13
| epoch   4 |  1400/ 2108 batches | lr 20.00 | ms/batch 169.52 | loss  8.27 | ppl  3908.73
| epoch   4 |  1600/ 2108 batches | lr 20.00 | ms/batch 170.32 | loss  8.21 | ppl  3675.51


| epoch   4 |  1800/ 2108 batches | lr 20.00 | ms/batch 170.00 | loss  8.19 | ppl  3613.79
| epoch   4 |  2000/ 2108 batches | lr 20.00 | ms/batch 170.22 | loss  8.25 | ppl  3841.54
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 381.69s | valid loss  9.03 | valid ppl  8352.72
-----------------------------------------------------------------------------------------
| epoch   5 |   200/ 2108 batches | lr 20.00 | ms/batch 170.58 | loss  8.21 | ppl  3691.31
| epoch   5 |   400/ 2108 batches | lr 20.00 | ms/batch 169.67 | loss  8.15 | ppl  3452.55
| epoch   5 |   600/ 2108 batches | lr 20.00 | ms/batch 170.22 | loss  7.90 | ppl  2685.60
| epoch   5 |   800/ 2108 batches | lr 20.00 | ms/batch 169.79 | loss  8.12 | ppl  3346.83
| epoch   5 |  1000/ 2108 batches | lr 20.00 | ms/batch 169.98 | loss  8.14 | ppl  3437.97
| epoch   5 |  1200/ 2108 batches | lr 20.00 | ms/batch 170.71 | loss  8.07 | ppl  3206.65
| epoch   5 |  1400/ 2108 batches | lr 20.00 | ms/batch 169.69 | loss  7.99 | ppl  2954.63
| epoch   5 |  1600/ 2108 batches | lr 20.00 | ms/batch 169.87 | loss  7.93 | ppl  2780.60


| epoch   5 |  1800/ 2108 batches | lr 20.00 | ms/batch 169.90 | loss  7.93 | ppl  2770.62
| epoch   5 |  2000/ 2108 batches | lr 20.00 | ms/batch 170.09 | loss  7.99 | ppl  2939.57
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 381.25s | valid loss  8.99 | valid ppl  7986.14
-----------------------------------------------------------------------------------------

| epoch   6 |   200/ 2108 batches | lr 20.00 | ms/batch 171.21 | loss  7.95 | ppl  2837.45

| epoch   6 |   400/ 2108 batches | lr 20.00 | ms/batch 170.56 | loss  7.89 | ppl  2671.53
| epoch   6 |   600/ 2108 batches | lr 20.00 | ms/batch 170.82 | loss  7.64 | ppl  2086.73
| epoch   6 |   800/ 2108 batches | lr 20.00 | ms/batch 170.05 | loss  7.88 | ppl  2640.66
| epoch   6 |  1000/ 2108 batches | lr 20.00 | ms/batch 170.18 | loss  7.89 | ppl  2677.26
| epoch   6 |  1200/ 2108 batches | lr 20.00 | ms/batch 170.43 | loss  7.83 | ppl  2517.85
| epoch   6 |  1400/ 2108 batches | lr 20.00 | ms/batch 169.99 | loss  7.75 | ppl  2315.63


| epoch   6 |  1600/ 2108 batches | lr 20.00 | ms/batch 170.51 | loss  7.69 | ppl  2194.41
| epoch   6 |  1800/ 2108 batches | lr 20.00 | ms/batch 170.44 | loss  7.69 | ppl  2181.40

| epoch   6 |  2000/ 2108 batches | lr 20.00 | ms/batch 170.38 | loss  7.75 | ppl  2313.36
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 382.21s | valid loss  9.00 | valid ppl  8088.95
-----------------------------------------------------------------------------------------

=========================================================================================
| End of training | test loss  8.98 | test ppl  7915.84
=========================================================================================
moisioa3@smith ~/snlp-project/word_language_model
 %
moisioa3@smith ~/snlp-project/word_language_model
 %
moisioa3@smith ~/snlp-project/word_language_model
 %
moisioa3@smith ~/snlp-project/word_language_model
 %
moisioa3@smith ~/snlp-project/word_language_model
 %
moisioa3@smith ~/snlp-project/word_language_model
 %
moisioa3@smith ~/snlp-project/word_language_model
 %
moisioa3@smith ~/snlp-project/word_language_model
 %
moisioa3@smith ~/snlp-project/word_language_model
 %
moisioa3@smith ~/snlp-project/word_language_model
 %
moisioa3@smith ~/snlp-project/word_language_model
 %
moipython3 generate.py --cuda --data yle --outf gener_w2v_wp-il_non_norm.txt --checkpoint w2v_wp-il_non_norm.pt
| Generated 0/1000 words
| Generated 100/1000 wordsg=1,negative=5,iter=10.bin --save w2v_wp-il_non_norm.pt
| Generated 200/1000 words
| Generated 300/1000 words
| Generated 400/1000 words
| Generated 500/1000 words
| Generated 600/1000 words
| Generated 700/1000 words
| Generated 800/1000 words
| Generated 900/1000 words
moisioa3@smith ~/snlp-project/word_language_model