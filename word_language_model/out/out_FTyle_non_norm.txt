moisioa3@smith ~/snlp-project/word_language_model
 % python3 main.py --cuda --data yle --emmodel ../data/embeddings/FastText_iltalehti_new_NON_normalized_size=200,alpha=0.025,window=10,min_count=2,sg=1,negative=5,iter=10,min_n=3,max_n=6.bin --save ft_yle_non_norm.pt
using pretrained word embeddings ../data/embeddings/FastText_iltalehti_new_NON_normalized_size=200,alpha=0.025,window=10,min_count=2,sg=1,negative=5,iter=10,min_n=3,max_n=6.bin
encoder layer shape torch.Size([379997, 200])
OOV count 0
| epoch   1 |   200/ 2108 batches | lr 20.00 | ms/batch 167.74 | loss 11.33 | ppl 83537.27
| epoch   1 |   400/ 2108 batches | lr 20.00 | ms/batch 167.73 | loss 10.36 | ppl 31544.94
| epoch   1 |   600/ 2108 batches | lr 20.00 | ms/batch 167.84 | loss  9.84 | ppl 18786.20
| epoch   1 |   800/ 2108 batches | lr 20.00 | ms/batch 168.04 | loss  9.67 | ppl 15911.24
| epoch   1 |  1000/ 2108 batches | lr 20.00 | ms/batch 168.51 | loss  9.55 | ppl 14114.05
| epoch   1 |  1200/ 2108 batches | lr 20.00 | ms/batch 167.78 | loss  9.33 | ppl 11266.19
| epoch   1 |  1400/ 2108 batches | lr 20.00 | ms/batch 168.56 | loss  9.22 | ppl 10117.58
| epoch   1 |  1600/ 2108 batches | lr 20.00 | ms/batch 169.11 | loss  9.11 | ppl  9077.00
| epoch   1 |  1800/ 2108 batches | lr 20.00 | ms/batch 169.72 | loss  9.03 | ppl  8348.34
| epoch   1 |  2000/ 2108 batches | lr 20.00 | ms/batch 170.86 | loss  9.04 | ppl  8403.23
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 378.32s | valid loss  9.03 | valid ppl  8357.24
-----------------------------------------------------------------------------------------
| epoch   2 |   200/ 2108 batches | lr 20.00 | ms/batch 170.97 | loss  8.90 | ppl  7340.44
| epoch   2 |   400/ 2108 batches | lr 20.00 | ms/batch 170.92 | loss  8.78 | ppl  6485.65
| epoch   2 |   600/ 2108 batches | lr 20.00 | ms/batch 170.52 | loss  8.48 | ppl  4813.15
| epoch   2 |   800/ 2108 batches | lr 20.00 | ms/batch 170.06 | loss  8.60 | ppl  5446.67
| epoch   2 |  1000/ 2108 batches | lr 20.00 | ms/batch 170.49 | loss  8.61 | ppl  5490.66
| epoch   2 |  1200/ 2108 batches | lr 20.00 | ms/batch 170.72 | loss  8.49 | ppl  4848.40
| epoch   2 |  1400/ 2108 batches | lr 20.00 | ms/batch 170.54 | loss  8.41 | ppl  4486.76
| epoch   2 |  1600/ 2108 batches | lr 20.00 | ms/batch 170.74 | loss  8.35 | ppl  4245.54
| epoch   2 |  1800/ 2108 batches | lr 20.00 | ms/batch 170.45 | loss  8.32 | ppl  4096.79
| epoch   2 |  2000/ 2108 batches | lr 20.00 | ms/batch 171.05 | loss  8.36 | ppl  4281.09
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 382.73s | valid loss  8.69 | valid ppl  5941.33
-----------------------------------------------------------------------------------------
| epoch   3 |   200/ 2108 batches | lr 20.00 | ms/batch 171.40 | loss  8.29 | ppl  4001.07
| epoch   3 |   400/ 2108 batches | lr 20.00 | ms/batch 169.57 | loss  8.21 | ppl  3692.06
| epoch   3 |   600/ 2108 batches | lr 20.00 | ms/batch 170.13 | loss  7.93 | ppl  2782.70
| epoch   3 |   800/ 2108 batches | lr 20.00 | ms/batch 170.14 | loss  8.11 | ppl  3336.87
| epoch   3 |  1000/ 2108 batches | lr 20.00 | ms/batch 170.49 | loss  8.13 | ppl  3396.69
| epoch   3 |  1200/ 2108 batches | lr 20.00 | ms/batch 170.22 | loss  8.03 | ppl  3080.36
| epoch   3 |  1400/ 2108 batches | lr 20.00 | ms/batch 170.81 | loss  7.95 | ppl  2825.82
| epoch   3 |  1600/ 2108 batches | lr 20.00 | ms/batch 171.12 | loss  7.90 | ppl  2695.00
| epoch   3 |  1800/ 2108 batches | lr 20.00 | ms/batch 171.04 | loss  7.88 | ppl  2630.81
| epoch   3 |  2000/ 2108 batches | lr 20.00 | ms/batch 170.31 | loss  7.93 | ppl  2786.31
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 382.32s | valid loss  8.61 | valid ppl  5511.79
-----------------------------------------------------------------------------------------
| epoch   4 |   200/ 2108 batches | lr 20.00 | ms/batch 171.46 | loss  7.89 | ppl  2657.52
| epoch   4 |   400/ 2108 batches | lr 20.00 | ms/batch 171.03 | loss  7.82 | ppl  2480.74
| epoch   4 |   600/ 2108 batches | lr 20.00 | ms/batch 171.00 | loss  7.54 | ppl  1885.66
| epoch   4 |   800/ 2108 batches | lr 20.00 | ms/batch 171.00 | loss  7.76 | ppl  2335.45
| epoch   4 |  1000/ 2108 batches | lr 20.00 | ms/batch 171.15 | loss  7.76 | ppl  2354.14
| epoch   4 |  1200/ 2108 batches | lr 20.00 | ms/batch 171.16 | loss  7.68 | ppl  2172.35
| epoch   4 |  1400/ 2108 batches | lr 20.00 | ms/batch 171.65 | loss  7.60 | ppl  1998.73
| epoch   4 |  1600/ 2108 batches | lr 20.00 | ms/batch 171.24 | loss  7.56 | ppl  1918.66
| epoch   4 |  1800/ 2108 batches | lr 20.00 | ms/batch 171.79 | loss  7.55 | ppl  1892.80
| epoch   4 |  2000/ 2108 batches | lr 20.00 | ms/batch 171.69 | loss  7.60 | ppl  2003.08
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 383.98s | valid loss  8.54 | valid ppl  5107.41
-----------------------------------------------------------------------------------------
| epoch   5 |   200/ 2108 batches | lr 20.00 | ms/batch 171.74 | loss  7.57 | ppl  1931.86
| epoch   5 |   400/ 2108 batches | lr 20.00 | ms/batch 171.02 | loss  7.50 | ppl  1813.63
| epoch   5 |   600/ 2108 batches | lr 20.00 | ms/batch 170.85 | loss  7.25 | ppl  1401.93
| epoch   5 |   800/ 2108 batches | lr 20.00 | ms/batch 170.79 | loss  7.47 | ppl  1747.19
| epoch   5 |  1000/ 2108 batches | lr 20.00 | ms/batch 169.77 | loss  7.47 | ppl  1762.82
| epoch   5 |  1200/ 2108 batches | lr 20.00 | ms/batch 170.98 | loss  7.41 | ppl  1645.20
| epoch   5 |  1400/ 2108 batches | lr 20.00 | ms/batch 170.50 | loss  7.32 | ppl  1505.93
| epoch   5 |  1600/ 2108 batches | lr 20.00 | ms/batch 171.27 | loss  7.28 | ppl  1458.12
| epoch   5 |  1800/ 2108 batches | lr 20.00 | ms/batch 171.51 | loss  7.27 | ppl  1437.24
| epoch   5 |  2000/ 2108 batches | lr 20.00 | ms/batch 170.07 | loss  7.33 | ppl  1524.15
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 382.98s | valid loss  8.51 | valid ppl  4976.71
-----------------------------------------------------------------------------------------
| epoch   6 |   200/ 2108 batches | lr 20.00 | ms/batch 171.22 | loss  7.30 | ppl  1478.14
| epoch   6 |   400/ 2108 batches | lr 20.00 | ms/batch 170.92 | loss  7.24 | ppl  1395.35
| epoch   6 |   600/ 2108 batches | lr 20.00 | ms/batch 170.69 | loss  6.99 | ppl  1083.26
| epoch   6 |   800/ 2108 batches | lr 20.00 | ms/batch 170.64 | loss  7.22 | ppl  1366.65
| epoch   6 |  1000/ 2108 batches | lr 20.00 | ms/batch 170.30 | loss  7.23 | ppl  1373.70
| epoch   6 |  1200/ 2108 batches | lr 20.00 | ms/batch 170.73 | loss  7.17 | ppl  1294.43
| epoch   6 |  1400/ 2108 batches | lr 20.00 | ms/batch 170.52 | loss  7.08 | ppl  1185.60
| epoch   6 |  1600/ 2108 batches | lr 20.00 | ms/batch 171.54 | loss  7.04 | ppl  1141.83
| epoch   6 |  1800/ 2108 batches | lr 20.00 | ms/batch 169.82 | loss  7.04 | ppl  1136.42
| epoch   6 |  2000/ 2108 batches | lr 20.00 | ms/batch 170.48 | loss  7.10 | ppl  1210.98
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 382.68s | valid loss  8.52 | valid ppl  5017.11
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  8.50 | test ppl  4905.87
=========================================================================================
moisioa3@smith ~/snlp-project/word_language_model
 % python3 generate.py --cuda --data yle --outf gener_ftyle_non_norm.txt --checkpoint ft_yle_non_norm.pt
| Generated 0/1000 words
| Generated 100/1000 words
| Generated 200/1000 words
| Generated 300/1000 words
| Generated 400/1000 words
| Generated 500/1000 words
| Generated 600/1000 words
| Generated 700/1000 words
| Generated 800/1000 words
| Generated 900/1000 words
moisioa3@smith ~/snlp-project/word_language_model