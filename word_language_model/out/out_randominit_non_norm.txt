moisioa3@ook ~/snlp-project/word_language_model
 % python3 main.py --cuda --data yle --emmodel no --save randominit_non_norm.pt
encoder layer shape torch.Size([379997, 200])
OOV count 0
| epoch   1 |   200/ 2108 batches | lr 20.00 | ms/batch 201.04 | loss 11.26 | ppl 77855.01
| epoch   1 |   400/ 2108 batches | lr 20.00 | ms/batch 199.58 | loss 10.62 | ppl 40912.65
| epoch   1 |   600/ 2108 batches | lr 20.00 | ms/batch 199.81 | loss 10.37 | ppl 32017.95
| epoch   1 |   800/ 2108 batches | lr 20.00 | ms/batch 199.27 | loss 10.25 | ppl 28327.19
| epoch   1 |  1000/ 2108 batches | lr 20.00 | ms/batch 198.66 | loss 10.19 | ppl 26570.68
| epoch   1 |  1200/ 2108 batches | lr 20.00 | ms/batch 198.60 | loss 10.02 | ppl 22535.00
| epoch   1 |  1400/ 2108 batches | lr 20.00 | ms/batch 199.25 | loss  9.98 | ppl 21666.55
| epoch   1 |  1600/ 2108 batches | lr 20.00 | ms/batch 198.46 | loss  9.90 | ppl 19859.19
| epoch   1 |  1800/ 2108 batches | lr 20.00 | ms/batch 198.46 | loss  9.83 | ppl 18555.03
| epoch   1 |  2000/ 2108 batches | lr 20.00 | ms/batch 197.01 | loss  9.78 | ppl 17742.84
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 447.29s | valid loss  9.85 | valid ppl 18940.24
-----------------------------------------------------------------------------------------
| epoch   2 |   200/ 2108 batches | lr 20.00 | ms/batch 199.48 | loss  9.72 | ppl 16615.04
| epoch   2 |   400/ 2108 batches | lr 20.00 | ms/batch 198.79 | loss  9.63 | ppl 15215.32
| epoch   2 |   600/ 2108 batches | lr 20.00 | ms/batch 198.88 | loss  9.46 | ppl 12815.52
| epoch   2 |   800/ 2108 batches | lr 20.00 | ms/batch 198.76 | loss  9.51 | ppl 13556.32
| epoch   2 |  1000/ 2108 batches | lr 20.00 | ms/batch 198.29 | loss  9.54 | ppl 13965.73
| epoch   2 |  1200/ 2108 batches | lr 20.00 | ms/batch 198.99 | loss  9.43 | ppl 12430.06
| epoch   2 |  1400/ 2108 batches | lr 20.00 | ms/batch 197.96 | loss  9.41 | ppl 12149.64
| epoch   2 |  1600/ 2108 batches | lr 20.00 | ms/batch 199.69 | loss  9.34 | ppl 11374.88
| epoch   2 |  1800/ 2108 batches | lr 20.00 | ms/batch 197.75 | loss  9.30 | ppl 10899.86
| epoch   2 |  2000/ 2108 batches | lr 20.00 | ms/batch 198.89 | loss  9.33 | ppl 11321.42
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 446.57s | valid loss  9.62 | valid ppl 15092.12
-----------------------------------------------------------------------------------------
| epoch   3 |   200/ 2108 batches | lr 20.00 | ms/batch 200.19 | loss  9.28 | ppl 10759.60
| epoch   3 |   400/ 2108 batches | lr 20.00 | ms/batch 198.44 | loss  9.22 | ppl 10104.30
| epoch   3 |   600/ 2108 batches | lr 20.00 | ms/batch 198.94 | loss  9.02 | ppl  8246.53
| epoch   3 |   800/ 2108 batches | lr 20.00 | ms/batch 200.03 | loss  9.14 | ppl  9295.45
| epoch   3 |  1000/ 2108 batches | lr 20.00 | ms/batch 198.80 | loss  9.19 | ppl  9777.59
| epoch   3 |  1200/ 2108 batches | lr 20.00 | ms/batch 198.05 | loss  9.09 | ppl  8826.77
| epoch   3 |  1400/ 2108 batches | lr 20.00 | ms/batch 198.74 | loss  9.04 | ppl  8472.55
| epoch   3 |  1600/ 2108 batches | lr 20.00 | ms/batch 198.99 | loss  8.99 | ppl  8044.26
| epoch   3 |  1800/ 2108 batches | lr 20.00 | ms/batch 198.36 | loss  8.95 | ppl  7696.74
| epoch   3 |  2000/ 2108 batches | lr 20.00 | ms/batch 198.46 | loss  9.01 | ppl  8203.40
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 446.79s | valid loss  9.37 | valid ppl 11678.26
-----------------------------------------------------------------------------------------
| epoch   4 |   200/ 2108 batches | lr 20.00 | ms/batch 199.93 | loss  8.96 | ppl  7800.72
| epoch   4 |   400/ 2108 batches | lr 20.00 | ms/batch 198.89 | loss  8.91 | ppl  7438.54
| epoch   4 |   600/ 2108 batches | lr 20.00 | ms/batch 198.80 | loss  8.68 | ppl  5882.94
| epoch   4 |   800/ 2108 batches | lr 20.00 | ms/batch 199.30 | loss  8.84 | ppl  6907.41
| epoch   4 |  1000/ 2108 batches | lr 20.00 | ms/batch 199.52 | loss  8.90 | ppl  7303.72
| epoch   4 |  1200/ 2108 batches | lr 20.00 | ms/batch 198.86 | loss  8.80 | ppl  6638.72
| epoch   4 |  1400/ 2108 batches | lr 20.00 | ms/batch 199.11 | loss  8.75 | ppl  6320.98
| epoch   4 |  1600/ 2108 batches | lr 20.00 | ms/batch 199.75 | loss  8.70 | ppl  5980.99
| epoch   4 |  1800/ 2108 batches | lr 20.00 | ms/batch 198.68 | loss  8.66 | ppl  5767.25
| epoch   4 |  2000/ 2108 batches | lr 20.00 | ms/batch 199.89 | loss  8.73 | ppl  6187.72
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 447.55s | valid loss  9.28 | valid ppl 10687.97
-----------------------------------------------------------------------------------------
| epoch   5 |   200/ 2108 batches | lr 20.00 | ms/batch 199.62 | loss  8.69 | ppl  5960.70
| epoch   5 |   400/ 2108 batches | lr 20.00 | ms/batch 198.71 | loss  8.65 | ppl  5713.73
| epoch   5 |   600/ 2108 batches | lr 20.00 | ms/batch 200.37 | loss  8.40 | ppl  4441.10
| epoch   5 |   800/ 2108 batches | lr 20.00 | ms/batch 201.44 | loss  8.59 | ppl  5366.85
| epoch   5 |  1000/ 2108 batches | lr 20.00 | ms/batch 200.13 | loss  8.63 | ppl  5615.33
| epoch   5 |  1200/ 2108 batches | lr 20.00 | ms/batch 200.58 | loss  8.55 | ppl  5158.19
| epoch   5 |  1400/ 2108 batches | lr 20.00 | ms/batch 198.96 | loss  8.49 | ppl  4874.30
| epoch   5 |  1600/ 2108 batches | lr 20.00 | ms/batch 201.01 | loss  8.44 | ppl  4618.53
| epoch   5 |  1800/ 2108 batches | lr 20.00 | ms/batch 201.00 | loss  8.41 | ppl  4493.99
| epoch   5 |  2000/ 2108 batches | lr 20.00 | ms/batch 200.84 | loss  8.47 | ppl  4770.22
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 449.77s | valid loss  9.21 | valid ppl 10016.93
-----------------------------------------------------------------------------------------
| epoch   6 |   200/ 2108 batches | lr 20.00 | ms/batch 201.88 | loss  8.44 | ppl  4618.66
| epoch   6 |   400/ 2108 batches | lr 20.00 | ms/batch 200.72 | loss  8.39 | ppl  4411.48
| epoch   6 |   600/ 2108 batches | lr 20.00 | ms/batch 202.73 | loss  8.15 | ppl  3454.03
| epoch   6 |   800/ 2108 batches | lr 20.00 | ms/batch 201.24 | loss  8.35 | ppl  4240.91
| epoch   6 |  1000/ 2108 batches | lr 20.00 | ms/batch 203.07 | loss  8.39 | ppl  4389.64
| epoch   6 |  1200/ 2108 batches | lr 20.00 | ms/batch 201.80 | loss  8.31 | ppl  4077.90
| epoch   6 |  1400/ 2108 batches | lr 20.00 | ms/batch 204.13 | loss  8.25 | ppl  3823.07
| epoch   6 |  1600/ 2108 batches | lr 20.00 | ms/batch 203.33 | loss  8.19 | ppl  3601.62
| epoch   6 |  1800/ 2108 batches | lr 20.00 | ms/batch 203.27 | loss  8.17 | ppl  3535.76
| epoch   6 |  2000/ 2108 batches | lr 20.00 | ms/batch 203.74 | loss  8.23 | ppl  3759.34
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 454.62s | valid loss  9.18 | valid ppl  9745.33
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  9.18 | test ppl  9725.05
=========================================================================================
moisioa3@ook ~/snlp-project/word_language_model
 % python3 generate.py --cuda --data yle --outf gener_randominit_non_norm.txt --checkpoint randominit_non_norm.pt    | Generated 0/1000 words
| Generated 100/1000 words
| Generated 200/1000 words
| Generated 300/1000 words
| Generated 400/1000 words
| Generated 500/1000 words
| Generated 600/1000 words
| Generated 700/1000 words
| Generated 800/1000 words
| Generated 900/1000 words
moisioa3@ook ~/snlp-project/word_language_model