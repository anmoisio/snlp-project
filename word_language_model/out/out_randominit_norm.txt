
 % python3 main.py --cuda --data yle_norm --emmodel no --save randominit_norm.pt
encoder layer shape torch.Size([142249, 200])
OOV count 0
| epoch   1 |   200/ 2605 batches | lr 20.00 | ms/batch 57.75 | loss  8.79 | ppl  6550.93
| epoch   1 |   400/ 2605 batches | lr 20.00 | ms/batch 57.11 | loss  7.67 | ppl  2141.39
| epoch   1 |   600/ 2605 batches | lr 20.00 | ms/batch 57.01 | loss  7.38 | ppl  1604.22
| epoch   1 |   800/ 2605 batches | lr 20.00 | ms/batch 56.89 | loss  7.20 | ppl  1332.94
| epoch   1 |  1000/ 2605 batches | lr 20.00 | ms/batch 56.91 | loss  7.06 | ppl  1161.18
| epoch   1 |  1200/ 2605 batches | lr 20.00 | ms/batch 56.73 | loss  7.01 | ppl  1106.84
| epoch   1 |  1400/ 2605 batches | lr 20.00 | ms/batch 56.83 | loss  6.93 | ppl  1019.62
| epoch   1 |  1600/ 2605 batches | lr 20.00 | ms/batch 56.97 | loss  6.83 | ppl   928.32
| epoch   1 |  1800/ 2605 batches | lr 20.00 | ms/batch 56.78 | loss  6.80 | ppl   897.73
| epoch   1 |  2000/ 2605 batches | lr 20.00 | ms/batch 56.78 | loss  6.73 | ppl   835.40
| epoch   1 |  2200/ 2605 batches | lr 20.00 | ms/batch 56.81 | loss  6.69 | ppl   806.38
| epoch   1 |  2400/ 2605 batches | lr 20.00 | ms/batch 56.84 | loss  6.73 | ppl   836.76
| epoch   1 |  2600/ 2605 batches | lr 20.00 | ms/batch 56.95 | loss  6.63 | ppl   755.81
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 158.04s | valid loss  6.74 | valid ppl   841.72
-----------------------------------------------------------------------------------------
| epoch   2 |   200/ 2605 batches | lr 20.00 | ms/batch 57.16 | loss  6.67 | ppl   788.58
| epoch   2 |   400/ 2605 batches | lr 20.00 | ms/batch 56.97 | loss  6.62 | ppl   747.58
| epoch   2 |   600/ 2605 batches | lr 20.00 | ms/batch 56.93 | loss  6.50 | ppl   663.56
| epoch   2 |   800/ 2605 batches | lr 20.00 | ms/batch 57.00 | loss  6.47 | ppl   644.15
| epoch   2 |  1000/ 2605 batches | lr 20.00 | ms/batch 57.02 | loss  6.50 | ppl   664.32
| epoch   2 |  1200/ 2605 batches | lr 20.00 | ms/batch 56.91 | loss  6.52 | ppl   676.50
| epoch   2 |  1400/ 2605 batches | lr 20.00 | ms/batch 56.94 | loss  6.49 | ppl   657.03
| epoch   2 |  1600/ 2605 batches | lr 20.00 | ms/batch 56.92 | loss  6.43 | ppl   617.21
| epoch   2 |  1800/ 2605 batches | lr 20.00 | ms/batch 57.03 | loss  6.38 | ppl   589.77
| epoch   2 |  2000/ 2605 batches | lr 20.00 | ms/batch 57.03 | loss  6.37 | ppl   583.71
| epoch   2 |  2200/ 2605 batches | lr 20.00 | ms/batch 56.98 | loss  6.34 | ppl   566.85
| epoch   2 |  2400/ 2605 batches | lr 20.00 | ms/batch 56.92 | loss  6.41 | ppl   609.66
| epoch   2 |  2600/ 2605 batches | lr 20.00 | ms/batch 56.96 | loss  6.33 | ppl   561.94
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 158.13s | valid loss  6.56 | valid ppl   705.71
-----------------------------------------------------------------------------------------
| epoch   3 |   200/ 2605 batches | lr 20.00 | ms/batch 57.32 | loss  6.39 | ppl   598.59
| epoch   3 |   400/ 2605 batches | lr 20.00 | ms/batch 56.94 | loss  6.34 | ppl   568.89
| epoch   3 |   600/ 2605 batches | lr 20.00 | ms/batch 57.01 | loss  6.22 | ppl   503.49
| epoch   3 |   800/ 2605 batches | lr 20.00 | ms/batch 57.00 | loss  6.18 | ppl   483.67
| epoch   3 |  1000/ 2605 batches | lr 20.00 | ms/batch 57.02 | loss  6.28 | ppl   531.66
| epoch   3 |  1200/ 2605 batches | lr 20.00 | ms/batch 57.12 | loss  6.27 | ppl   530.29
| epoch   3 |  1400/ 2605 batches | lr 20.00 | ms/batch 57.05 | loss  6.27 | ppl   526.70
| epoch   3 |  1600/ 2605 batches | lr 20.00 | ms/batch 57.01 | loss  6.20 | ppl   494.91
| epoch   3 |  1800/ 2605 batches | lr 20.00 | ms/batch 57.03 | loss  6.15 | ppl   468.45
| epoch   3 |  2000/ 2605 batches | lr 20.00 | ms/batch 57.03 | loss  6.15 | ppl   468.38
| epoch   3 |  2200/ 2605 batches | lr 20.00 | ms/batch 57.13 | loss  6.13 | ppl   460.37
| epoch   3 |  2400/ 2605 batches | lr 20.00 | ms/batch 57.02 | loss  6.21 | ppl   497.82
| epoch   3 |  2600/ 2605 batches | lr 20.00 | ms/batch 56.99 | loss  6.14 | ppl   464.00
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 158.28s | valid loss  6.48 | valid ppl   653.69
-----------------------------------------------------------------------------------------
| epoch   4 |   200/ 2605 batches | lr 20.00 | ms/batch 57.35 | loss  6.21 | ppl   495.81
| epoch   4 |   400/ 2605 batches | lr 20.00 | ms/batch 57.18 | loss  6.15 | ppl   468.62
| epoch   4 |   600/ 2605 batches | lr 20.00 | ms/batch 57.13 | loss  6.04 | ppl   417.98
| epoch   4 |   800/ 2605 batches | lr 20.00 | ms/batch 57.00 | loss  6.00 | ppl   401.72
| epoch   4 |  1000/ 2605 batches | lr 20.00 | ms/batch 57.00 | loss  6.12 | ppl   454.61
| epoch   4 |  1200/ 2605 batches | lr 20.00 | ms/batch 56.98 | loss  6.11 | ppl   448.39
| epoch   4 |  1400/ 2605 batches | lr 20.00 | ms/batch 57.06 | loss  6.11 | ppl   452.33
| epoch   4 |  1600/ 2605 batches | lr 20.00 | ms/batch 57.12 | loss  6.05 | ppl   423.38
| epoch   4 |  1800/ 2605 batches | lr 20.00 | ms/batch 57.18 | loss  5.99 | ppl   397.74
| epoch   4 |  2000/ 2605 batches | lr 20.00 | ms/batch 57.17 | loss  6.00 | ppl   403.21
| epoch   4 |  2200/ 2605 batches | lr 20.00 | ms/batch 57.02 | loss  5.98 | ppl   396.01
| epoch   4 |  2400/ 2605 batches | lr 20.00 | ms/batch 57.09 | loss  6.06 | ppl   428.39
| epoch   4 |  2600/ 2605 batches | lr 20.00 | ms/batch 57.10 | loss  6.00 | ppl   404.70
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 158.43s | valid loss  6.40 | valid ppl   600.57
-----------------------------------------------------------------------------------------
| epoch   5 |   200/ 2605 batches | lr 20.00 | ms/batch 57.36 | loss  6.06 | ppl   429.48
| epoch   5 |   400/ 2605 batches | lr 20.00 | ms/batch 57.07 | loss  6.01 | ppl   408.77
| epoch   5 |   600/ 2605 batches | lr 20.00 | ms/batch 57.04 | loss  5.89 | ppl   361.78
| epoch   5 |   800/ 2605 batches | lr 20.00 | ms/batch 57.24 | loss  5.86 | ppl   349.27
| epoch   5 |  1000/ 2605 batches | lr 20.00 | ms/batch 57.08 | loss  6.00 | ppl   403.55
| epoch   5 |  1200/ 2605 batches | lr 20.00 | ms/batch 57.08 | loss  5.98 | ppl   394.15
| epoch   5 |  1400/ 2605 batches | lr 20.00 | ms/batch 57.06 | loss  5.99 | ppl   399.29
| epoch   5 |  1600/ 2605 batches | lr 20.00 | ms/batch 57.06 | loss  5.92 | ppl   371.01
| epoch   5 |  1800/ 2605 batches | lr 20.00 | ms/batch 57.19 | loss  5.86 | ppl   351.14
| epoch   5 |  2000/ 2605 batches | lr 20.00 | ms/batch 57.20 | loss  5.88 | ppl   357.85
| epoch   5 |  2200/ 2605 batches | lr 20.00 | ms/batch 57.06 | loss  5.86 | ppl   351.30
| epoch   5 |  2400/ 2605 batches | lr 20.00 | ms/batch 57.00 | loss  5.94 | ppl   379.63
| epoch   5 |  2600/ 2605 batches | lr 20.00 | ms/batch 57.08 | loss  5.89 | ppl   360.05
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 158.54s | valid loss  6.33 | valid ppl   561.23
-----------------------------------------------------------------------------------------
| epoch   6 |   200/ 2605 batches | lr 20.00 | ms/batch 57.48 | loss  5.95 | ppl   382.25
| epoch   6 |   400/ 2605 batches | lr 20.00 | ms/batch 57.03 | loss  5.89 | ppl   362.75
| epoch   6 |   600/ 2605 batches | lr 20.00 | ms/batch 57.05 | loss  5.77 | ppl   321.00
| epoch   6 |   800/ 2605 batches | lr 20.00 | ms/batch 57.03 | loss  5.73 | ppl   308.91
| epoch   6 |  1000/ 2605 batches | lr 20.00 | ms/batch 57.15 | loss  5.90 | ppl   364.12
| epoch   6 |  1200/ 2605 batches | lr 20.00 | ms/batch 57.08 | loss  5.87 | ppl   354.18
| epoch   6 |  1400/ 2605 batches | lr 20.00 | ms/batch 57.02 | loss  5.89 | ppl   360.29
| epoch   6 |  1600/ 2605 batches | lr 20.00 | ms/batch 57.03 | loss  5.81 | ppl   334.95
| epoch   6 |  1800/ 2605 batches | lr 20.00 | ms/batch 57.03 | loss  5.76 | ppl   316.00
| epoch   6 |  2000/ 2605 batches | lr 20.00 | ms/batch 57.05 | loss  5.77 | ppl   320.55
| epoch   6 |  2200/ 2605 batches | lr 20.00 | ms/batch 57.19 | loss  5.76 | ppl   316.37
| epoch   6 |  2400/ 2605 batches | lr 20.00 | ms/batch 57.19 | loss  5.84 | ppl   342.58
| epoch   6 |  2600/ 2605 batches | lr 20.00 | ms/batch 57.12 | loss  5.79 | ppl   325.86
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 158.44s | valid loss  6.30 | valid ppl   544.66
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  6.28 | test ppl   534.87
=========================================================================================
moisioa3@bogo ~/snlp-project/word_language_model
 % python3 generate.py --cuda --data yle_norm --save gener_randominit_norm.txt
usage: generate.py [-h] [--data DATA] [--checkpoint CHECKPOINT] [--outf OUTF]
                   [--words WORDS] [--seed SEED] [--cuda]
                   [--temperature TEMPERATURE] [--log-interval LOG_INTERVAL]
generate.py: error: unrecognized arguments: --save gener_randominit_norm.txt
moisioa3@bogo ~/snlp-project/word_language_model
 % python3 generate.py --cuda --data yle_norm --outf gener_randominit_norm.txt --checkpoint randominit_norm.pt
| Generated 0/1000 words
| Generated 100/1000 words
| Generated 200/1000 words
| Generated 300/1000 words
| Generated 400/1000 words
| Generated 500/1000 words
| Generated 600/1000 words
| Generated 700/1000 words
| Generated 800/1000 words
| Generated 900/1000 words
moisioa3@bogo ~/snlp-project/word_language_model