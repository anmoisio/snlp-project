oisioa3@bogo ~/snlp-project/word_language_model
 % python3 main.py --cuda --data yle --emmodel ../data/embeddings/Word2Vec_iltalehti_new_NON_normalized_size=200,alpha=0.025,window=10,min_count=2,sg=1,negative=5,iter=10.bin --save w2v_non_norm_il_non_norm.pt
using pretrained word embeddings ../data/embeddings/Word2Vec_iltalehti_new_NON_normalized_size=200,alpha=0.025,window=10,min_count=2,sg=1,negative=5,iter=10.bin
encoder layer shape torch.Size([379997, 200])
OOV count 262624
| epoch   1 |   200/ 2108 batches | lr 20.00 | ms/batch 166.02 | loss 11.21 | ppl 74181.15
| epoch   1 |   400/ 2108 batches | lr 20.00 | ms/batch 165.34 | loss 10.24 | ppl 28008.09
| epoch   1 |   600/ 2108 batches | lr 20.00 | ms/batch 166.42 | loss  9.87 | ppl 19370.68
| epoch   1 |   800/ 2108 batches | lr 20.00 | ms/batch 166.38 | loss  9.74 | ppl 16985.68
| epoch   1 |  1000/ 2108 batches | lr 20.00 | ms/batch 166.77 | loss  9.66 | ppl 15638.43
| epoch   1 |  1200/ 2108 batches | lr 20.00 | ms/batch 166.88 | loss  9.46 | ppl 12813.54
| epoch   1 |  1400/ 2108 batches | lr 20.00 | ms/batch 167.77 | loss  9.37 | ppl 11754.34
| epoch   1 |  1600/ 2108 batches | lr 20.00 | ms/batch 167.71 | loss  9.25 | ppl 10412.59
| epoch   1 |  1800/ 2108 batches | lr 20.00 | ms/batch 167.77 | loss  9.18 | ppl  9718.82
| epoch   1 |  2000/ 2108 batches | lr 20.00 | ms/batch 167.71 | loss  9.19 | ppl  9756.26
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 374.70s | valid loss  9.29 | valid ppl 10819.76
-----------------------------------------------------------------------------------------
| epoch   2 |   200/ 2108 batches | lr 20.00 | ms/batch 168.06 | loss  9.05 | ppl  8479.97
| epoch   2 |   400/ 2108 batches | lr 20.00 | ms/batch 167.80 | loss  8.93 | ppl  7533.06
| epoch   2 |   600/ 2108 batches | lr 20.00 | ms/batch 167.95 | loss  8.67 | ppl  5839.62
| epoch   2 |   800/ 2108 batches | lr 20.00 | ms/batch 167.65 | loss  8.80 | ppl  6640.78
| epoch   2 |  1000/ 2108 batches | lr 20.00 | ms/batch 167.85 | loss  8.81 | ppl  6724.76

| epoch   2 |  1200/ 2108 batches | lr 20.00 | ms/batch 168.06 | loss  8.69 | ppl  5947.27
| epoch   2 |  1400/ 2108 batches | lr 20.00 | ms/batch 168.49 | loss  8.62 | ppl  5551.49
| epoch   2 |  1600/ 2108 batches | lr 20.00 | ms/batch 168.98 | loss  8.55 | ppl  5174.63
| epoch   2 |  1800/ 2108 batches | lr 20.00 | ms/batch 169.39 | loss  8.51 | ppl  4976.70
| epoch   2 |  2000/ 2108 batches | lr 20.00 | ms/batch 168.64 | loss  8.57 | ppl  5248.32
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 377.60s | valid loss  8.91 | valid ppl  7427.24
-----------------------------------------------------------------------------------------
| epoch   3 |   200/ 2108 batches | lr 20.00 | ms/batch 168.99 | loss  8.49 | ppl  4854.79


| epoch   3 |   400/ 2108 batches | lr 20.00 | ms/batch 168.17 | loss  8.39 | ppl  4421.01
| epoch   3 |   600/ 2108 batches | lr 20.00 | ms/batch 168.55 | loss  8.14 | ppl  3438.06
| epoch   3 |   800/ 2108 batches | lr 20.00 | ms/batch 167.73 | loss  8.32 | ppl  4111.16
| epoch   3 |  1000/ 2108 batches | lr 20.00 | ms/batch 169.38 | loss  8.34 | ppl  4175.99
| epoch   3 |  1200/ 2108 batches | lr 20.00 | ms/batch 168.66 | loss  8.25 | ppl  3811.46
| epoch   3 |  1400/ 2108 batches | lr 20.00 | ms/batch 168.44 | loss  8.17 | ppl  3525.91
| epoch   3 |  1600/ 2108 batches | lr 20.00 | ms/batch 168.56 | loss  8.12 | ppl  3345.59
| epoch   3 |  1800/ 2108 batches | lr 20.00 | ms/batch 168.95 | loss  8.08 | ppl  3220.68
| epoch   3 |  2000/ 2108 batches | lr 20.00 | ms/batch 168.72 | loss  8.15 | ppl  3448.35
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 378.22s | valid loss  8.78 | valid ppl  6535.15
-----------------------------------------------------------------------------------------
| epoch   4 |   200/ 2108 batches | lr 20.00 | ms/batch 169.61 | loss  8.09 | ppl  3262.89
| epoch   4 |   400/ 2108 batches | lr 20.00 | ms/batch 168.52 | loss  8.01 | ppl  3010.79
| epoch   4 |   600/ 2108 batches | lr 20.00 | ms/batch 168.39 | loss  7.76 | ppl  2352.90
| epoch   4 |   800/ 2108 batches | lr 20.00 | ms/batch 168.07 | loss  7.96 | ppl  2872.14
| epoch   4 |  1000/ 2108 batches | lr 20.00 | ms/batch 168.90 | loss  7.99 | ppl  2941.66
| epoch   4 |  1200/ 2108 batches | lr 20.00 | ms/batch 168.85 | loss  7.90 | ppl  2696.49
| epoch   4 |  1400/ 2108 batches | lr 20.00 | ms/batch 168.88 | loss  7.82 | ppl  2502.30
| epoch   4 |  1600/ 2108 batches | lr 20.00 | ms/batch 168.22 | loss  7.77 | ppl  2377.17
| epoch   4 |  1800/ 2108 batches | lr 20.00 | ms/batch 168.96 | loss  7.75 | ppl  2325.50
| epoch   4 |  2000/ 2108 batches | lr 20.00 | ms/batch 168.46 | loss  7.83 | ppl  2503.36

-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 378.42s | valid loss  8.75 | valid ppl  6321.15
-----------------------------------------------------------------------------------------
| epoch   5 |   200/ 2108 batches | lr 20.00 | ms/batch 169.33 | loss  7.78 | ppl  2387.78
| epoch   5 |   400/ 2108 batches | lr 20.00 | ms/batch 169.13 | loss  7.71 | ppl  2220.70
| epoch   5 |   600/ 2108 batches | lr 20.00 | ms/batch 168.93 | loss  7.46 | ppl  1740.18
| epoch   5 |   800/ 2108 batches | lr 20.00 | ms/batch 168.64 | loss  7.68 | ppl  2154.25
| epoch   5 |  1000/ 2108 batches | lr 20.00 | ms/batch 168.96 | loss  7.69 | ppl  2186.27
| epoch   5 |  1200/ 2108 batches | lr 20.00 | ms/batch 168.38 | loss  7.62 | ppl  2031.36
| epoch   5 |  1400/ 2108 batches | lr 20.00 | ms/batch 168.41 | loss  7.54 | ppl  1881.93


| epoch   5 |  1600/ 2108 batches | lr 20.00 | ms/batch 168.09 | loss  7.49 | ppl  1798.88
| epoch   5 |  1800/ 2108 batches | lr 20.00 | ms/batch 169.02 | loss  7.48 | ppl  1767.33


| epoch   5 |  2000/ 2108 batches | lr 20.00 | ms/batch 168.58 | loss  7.55 | ppl  1904.99
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 378.61s | valid loss  8.73 | valid ppl  6170.32
-----------------------------------------------------------------------------------------
| epoch   6 |   200/ 2108 batches | lr 20.00 | ms/batch 169.49 | loss  7.51 | ppl  1818.42

| epoch   6 |   400/ 2108 batches | lr 20.00 | ms/batch 168.71 | loss  7.44 | ppl  1708.60
| epoch   6 |   600/ 2108 batches | lr 20.00 | ms/batch 168.93 | loss  7.21 | ppl  1347.99
| epoch   6 |   800/ 2108 batches | lr 20.00 | ms/batch 168.37 | loss  7.42 | ppl  1668.17
| epoch   6 |  1000/ 2108 batches | lr 20.00 | ms/batch 168.48 | loss  7.43 | ppl  1693.18
| epoch   6 |  1200/ 2108 batches | lr 20.00 | ms/batch 168.91 | loss  7.37 | ppl  1583.27
| epoch   6 |  1400/ 2108 batches | lr 20.00 | ms/batch 169.44 | loss  7.29 | ppl  1464.70
| epoch   6 |  1600/ 2108 batches | lr 20.00 | ms/batch 168.58 | loss  7.25 | ppl  1414.50
| epoch   6 |  1800/ 2108 batches | lr 20.00 | ms/batch 168.67 | loss  7.25 | ppl  1403.64
| epoch   6 |  2000/ 2108 batches | lr 20.00 | ms/batch 168.66 | loss  7.32 | ppl  1506.59

-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 378.80s | valid loss  8.75 | valid ppl  6296.82
-----------------------------------------------------------------------------------------
=========================================================================================
| End of training | test loss  8.71 | test ppl  6075.44
=========================================================================================
moisioa3@bogo ~/snlp-project/word_language_model
 %
moisioa3@bogo ~/snlp-project/word_language_model
 %
moisioa3@bogo ~/snlp-project/word_language_model
 %
moisioa3@bogo ~/snlp-project/word_language_model
 %
moisioa3@bogo ~/snlp-project/word_language_model
 %
moisioa3@bogo ~/snlp-project/word_language_model
 %
moisioa3@bogo ~/snlp-project/word_language_model
 %
moisioa3@bogo ~/snlp-project/word_language_model
 %
moisioa3@bogo ~/snlp-project/word_language_model
 %
moisioa3@bogo ~/snlp-project/word_language_model
 %
moisioa3@bogo ~/snlp-project/word_language_model
 % python3 generate.py --cuda --data yle --outf gener_w2v_non_norm_il_non_norm.txt --checkpoint w2v_non_norm_il_non_norm.pt                                                                           | Generated 0/1000 words
| Generated 100/1000 words
| Generated 200/1000 words
| Generated 300/1000 words
| Generated 400/1000 words
| Generated 500/1000 words
| Generated 600/1000 words
| Generated 700/1000 words
| Generated 800/1000 words
| Generated 900/1000 words
moisioa3@bogo ~/snlp-project/word_language_model